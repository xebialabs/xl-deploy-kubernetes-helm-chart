# Default values for xl-deploy.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Platform on which to install the chart (PlainK8s/AWSEKS/AzureAKS/GoogleGKE)
K8sSetup:
  Platform: PlainK8s

# No. of XL-Deploy master and worker pods to run.
XldMasterCount: 3
XldWorkerCount: 3

## XL-Deploy image version
## Ref: https://hub.docker.com/r/xebialabs/xl-deploy/tags
ServerImageRepository: "xebialabs/xl-deploy"
WorkerImageRepository: "xebialabs/deploy-task-engine"
ImageTag: "22.2"

## xebialabs/tiny-tools image version
## Ref: https://hub.docker.com/r/xebialabs/tiny-tools/tags
TinyToolsImageRepository: "xebialabs/tiny-tools"
TinyToolsImageTag: "22.2.0"

## Specify a imagePullPolicy
## Defaults to 'Always' if image tag is 'latest',set to 'IfNotPresent'
ImagePullPolicy: "Always"

## Secrets must be manually created in the namespace.
# ImagePullSecret: xlDeploy

deploy:
  clusterMode: full
  master:
    useIpAsHostname: false
    clusterNodeHostnameSuffix: '.{{ template "xl-deploy.fullname" . }}-master.{{.Release.Namespace}}.svc.cluster.local'
    terminationGracePeriodSeconds: 200
    image:
      tag: ""
    ## Kubernetes service type that is created for each master Pod in StatefulSet
    ##
    podServiceTemplate:
      enabled: false
      ## @param service.type Kubernetes Service type
      ##
      type: NodePort

      ## @param service.name the name of the service, the service name is composed of release version name, fix part '-master-' and the order number of the service, the helm is passing the `.podNumber` value
      name: '{{ printf "%s-master-" (include "xl-deploy.fullname" $) }}{{ .podNumber }}'
      ## @param service.serviceMode Possible values are: SingleHostname (IncrementPort, MultiService), SinglePort (IncrementHostname, MultiService), MultiService (IncrementHostname, IncrementPort), SingleService (IncrementHostname, SinglePort)
      ## it defines the number of hostnames, ports and services
      serviceMode: MultiService
      ## @param service.overrideHostnameSuffix together with overrideHostname composes full hostname of the exposed master pod
      overrideHostnameSuffix: '.{{.Release.Namespace}}.svc.cluster.local'
      ## @param service.overrideHostname  together with overrideHostnameSuffix composes full hostname of the exposed master pod
      overrideHostname: '{{ printf "%s-master-" (include "xl-deploy.fullname" $) }}{{ .podNumber }}'
      overrideHostnames: []

      ## @param service.portEnabled deploy port. Cannot be disabled when `auth.tls.enabled` is `false`. Listener can be disabled with `listeners.tcp = none`.
      ##
      portEnabled: true
      ## Service ports
      ##
      ports:
        deployAkka: 32180
      ## Service ports name
      ##
      portNames:
        deployAkka: "akka"

      ## Node ports to expose
      ##
      nodePorts:
        deployAkka: 32180
      ## @param service.extraPorts Extra ports to expose in the service
      ## E.g.:
      ## extraPorts:
      ## - name: new_svc_name
      ##   port: 1234
      ##   targetPort: 1234
      ##
      extraPorts: [ ]
      ## @param service.loadBalancerSourceRanges Address(es) that are allowed when service is `LoadBalancer`
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: [ ]
      ## @param service.externalIPs Set the ExternalIPs
      ##
      externalIPs: [ ]
      ## @param service.externalTrafficPolicy Enable client source IP preservation
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Local
      ## @param service.clusterIPs Kubernetes service Cluster IP
      ## e.g.:
      ## clusterIP: [ None ]
      ##
      clusterIPs: [ ]
      ## @param service.annotations Service annotations. Evaluated as a template
      ## Example:
      ## annotations:
      ##   service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
      ##
      annotations: { }
      publishNotReadyAddresses: true
      ## @param service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
      ## If "ClientIP", consecutive client requests will be directed to the same Pod
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
      ##
      sessionAffinity: None
      ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: { }
      podLabels:
        statefulset.kubernetes.io/pod-name: '{{ printf "%s-master-%d" (include "xl-deploy.fullname" $) .podNumber }}'
  worker:
    useIpAsHostname: false
    terminationGracePeriodSeconds: 200
    image:
      tag: ""
    ## Kubernetes service type that is created for each worker Pod in StatefulSet
    ##
    podServiceTemplate:
      enabled: false
      ## @param service.type Kubernetes Service type
      ##
      type: NodePort

      ## @param service.name the name of the service, the service name is composed of release version name, fix part '-worker-' and the order number of the service, the helm is passing the `.podNumber` value
      name: '{{ printf "%s-worker-" (include "xl-deploy.fullname" $) }}{{ .podNumber }}'
      ## @param service.serviceMode Possible values are: SingleHostname (IncrementPort, MultiService), SinglePort (IncrementHostname, MultiService), MultiService (IncrementHostname, IncrementPort), SingleService (IncrementHostname, SinglePort)
      ## it defines the number of hostnames, ports and services
      serviceMode: MultiService
      ## @param service.overrideHostnameSuffix together with overrideHostname composes full hostname of the exposed worker pod
      overrideHostnameSuffix: '.{{.Release.Namespace}}.svc.cluster.local'
      ## @param service.overrideHostname  together with overrideHostnameSuffix composes full hostname of the exposed worker pod
      overrideHostname: '{{ printf "%s-worker-" (include "xl-deploy.fullname" $) }}{{ .podNumber }}'
      overrideHostnames: []

      ## @param service.portEnabled deploy port. Cannot be disabled when `auth.tls.enabled` is `false`. Listener can be disabled with `listeners.tcp = none`.
      ##
      portEnabled: true
      ## Service ports
      ##
      ports:
        deployAkka: 32185
      ## Service ports name
      ##
      portNames:
        deployAkka: "akka"

      ## Node ports to expose
      ##
      nodePorts:
        deployAkka: 32185
      ## @param service.extraPorts Extra ports to expose in the service
      ## E.g.:
      ## extraPorts:
      ## - name: new_svc_name
      ##   port: 1234
      ##   targetPort: 1234
      ##
      extraPorts: [ ]
      ## @param service.loadBalancerSourceRanges Address(es) that are allowed when service is `LoadBalancer`
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: [ ]
      ## @param service.externalIPs Set the ExternalIPs
      ##
      externalIPs: [ ]
      ## @param service.externalTrafficPolicy Enable client source IP preservation
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Local
      ## @param service.clusterIPs Kubernetes service Cluster IP
      ## e.g.:
      ## clusterIPs: [ None ]
      ##
      clusterIPs: [ ]
      ## @param service.annotations Service annotations. Evaluated as a template
      ## Example:
      ## annotations:
      ##   service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
      ##
      annotations: { }
      publishNotReadyAddresses: true
      ## @param service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
      ## If "ClientIP", consecutive client requests will be directed to the same Pod
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
      ##
      sessionAffinity: None
      ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: { }
      podLabels:
        statefulset.kubernetes.io/pod-name: '{{ printf "%s-worker-%d" (include "xl-deploy.fullname" $) .podNumber }}'
  centralConfiguration:
    useIpAsHostname: false
    terminationGracePeriodSeconds: 30
  configurationManagement:
    centralConfiguration:
      configuration:
        enabled: true
        resetFiles: ## Resets part of the configuration files
          - xlc-wrapper.conf.common
          - jmx-exporter.yaml
        resetCcFiles:
          - deploy-server.yaml
        script: |-
          cp /opt/xebialabs/central-configuration-server/xld-configuration-management/deploy-server.yaml.template /opt/xebialabs/central-configuration-server/central-conf/deploy-server.yaml.template && echo "Copy central-conf/deploy-server.yaml.template";
        scriptData: ## Add here files that will be used by the configurationScript
          deploy-server.yaml.template: |-
            deploy.server:
              bind-hostname: 0.0.0.0
              bind-port: 8180
              license:
                daysBeforeWarning: 10
      environment:
        enabled: true
        variables: {}
      volumeMounts:
        enabled: true
        paths: {}
    master:
      configuration:
        enabled: true
        resetFiles: ## Resets part of the configuration files and license
          - deployit-license.lic
          - xld-wrapper.conf.common
          - jmx-exporter.yaml
        script: |-
          cat example.txt && echo ""; # Add here commands to be executed before master start
        scriptData: ## Add here files that will be used by the configurationScript
          example.txt: |-
            Nothing to manage at this point.
      environment:
        enabled: true
        variables: {}
      volumeMounts:
        enabled: true
        paths: {}
    worker:
      configuration:
        enabled: true
        resetFiles: ## Resets part of the configuration files and license
          - deployit-license.lic
          - xld-wrapper.conf.common
          - jmx-exporter.yaml
        script: |-
          cat example.txt && echo ""; # Add here commands to be executed before worker start
        scriptData: ## Add here files that will be used by the configurationScript
          example.txt: |-
            Nothing to manage at this point.
      environment:
        enabled: true
        variables: {}
      volumeMounts:
        enabled: true
        paths: {}

## Install haproxy subchart. If you have haproxy already installed, set 'install' to 'false'.
## If you have any other ingress controller installed, you can set the 'install' to 'false'.
haproxy-ingress:
  install: true
  controller:
    imagePullSecrets: []
    kind: DaemonSet
    service:
      type: NodePort

nginx-ingress-controller:
  install: false

## Ingress Configuration
## Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  Enabled: true
  annotations:
    # ingress.kubernetes.io/tls-acme: "true"
    ingress.kubernetes.io/ssl-redirect: "false"
    kubernetes.io/ingress.class: haproxy-dai-xld
    ingress.kubernetes.io/rewrite-target: /
    ingress.kubernetes.io/affinity: cookie
    ingress.kubernetes.io/session-cookie-name: SESSION_XLD
    ingress.kubernetes.io/session-cookie-strategy: prefix
    ingress.kubernetes.io/config-backend: |
      option httpchk GET /deployit/ha/health HTTP/1.0
  path: /
  hosts:
    - example.com

#  tls:
#    - secretName: example-secretsName
#      hosts:
#        - example.com

# https://docs.xebialabs.com/v.10.2/deploy/docker/docker-environment-variables/
AdminPassword:
  # Provide the admin password to be used

xldLicense:
# Convert xl-deploy.lic files content to base64 ( cat xl-deploy.lic | base64 -w 0 ) and put the output here

RepositoryKeystore:
# https://docs.xebialabs.com/v.10.2/deploy/how-to/changing-passwords-in-xl-deploy/
# Convert repository-keystore.jceks files content to base64 ( cat repository-keystore.jceks | base64 -w 0 ) and put the output here

KeystorePassphrase:
# Passphrase for repository-keystore.jceks file

CentralConfigEncryptKey:
# spring cloud config encryption key

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 3
  #  memory: 3Gi
  # requests:
  #  cpu: 0.7
  #  memory: 1700Mi

## Ref: https://github.com/bitnami/charts/blob/master/bitnami/postgresql/README.md
## Install postgresql chart. If you have an existing database deployment, set 'install' to 'false'.
postgresql:
  install: true
  initdbScriptsSecret: "postgresql-init-sql-xld"
  image:
    pullSecrets: []
  persistence:
    enabled: true
    storageClass: "-"
    size: 8Gi
    existingClaim: ""
  postgresqlUsername: postgres
  postgresqlPassword: ""
  postgresqlMaxConnections: "300"
  service:
    type: ClusterIP
    port: 5432
  resources:
    requests:
      memory: 256Mi
      cpu: 250m

# https://docs.xebialabs.com/v.10.2/deploy/how-to/configure-the-xl-deploy-sql-repository/
UseExistingDB:
  Enabled: false
  # If you want to use an existing database, set 'postgresql.install' to 'false' and 'UseExistingDB.Enabled' to 'true'.
  # Uncomment the following lines and provide the values.
  # XL_DB_URL:
  # XL_DB_USERNAME:
  # XL_DB_PASSWORD:

## Install rabbitmq chart. If you have an existing message queue deployment, set 'install' to 'false'.
## ref: https://github.com/bitnami/charts/blob/master/bitnami/rabbitmq/README.md
rabbitmq:
  install: true
  replicaCount: 3
  image:
    pullSecrets: []
  auth:
    username: guest
    password: guest
    erlangCookie: DEPLOYRABBITMQCLUSTER
  extraPlugins: 'rabbitmq_jms_topic_exchange'
  extraSecrets:
    xld-load-definition:
      xld-load_definition.json: |
        {
        "users": [
          {
              "name": "{{ .Values.auth.username }}",
              "password": "{{ .Values.auth.password }}",
              "tags": "administrator"
          }
          ],
        "vhosts": [
          {
            "name": "/"
          }
          ],
          "permissions": [
          {
            "user": "{{ .Values.auth.username }}",
            "vhost": "/",
            "configure": ".*",
            "write": ".*",
            "read": ".*"
          }
          ],
        "global_parameters": [
          {
            "name": "cluster_name",
            "value": ""
          }
          ],
         "policies": [
            {
              "name": "ha-all",
              "apply-to": "all",
              "pattern": ".*",
              "vhost": "/",
              "definition": {
                "ha-mode": "all",
                "ha-sync-mode": "automatic",
                "ha-sync-batch-size": 1
              }
            }
          ]
        }
  loadDefinition:
    enabled: true
    existingSecret: xld-load-definition
  extraConfiguration: |
    load_definitions = /app/xld-load_definition.json
    raft.wal_max_size_bytes = 1048576
  persistence:
    enabled: true
    storageClass: "-"
    size: 8Gi
  service:
    type: ClusterIP
  volumePermissions:
    enabled: true

# https://docs.xebialabs.com/v.10.2/deploy/how-to/configure-task-queueing/
UseExistingMQ:
  Enabled: false
  # If you want to use a existing Message Queue, change the "rabbitmq.install to false"
  # Set 'UseExistingMQ.Enabled' to 'true'.Uncomment the following lines and provide the values.
  # XLD_TASK_QUEUE_USERNAME:
  # XLD_TASK_QUEUE_PASSWORD:
  # XLD_TASK_QUEUE_URL:
  # XLD_TASK_QUEUE_DRIVER_CLASS_NAME:

## Configure extra options for liveness and readiness probes
HealthProbes: true
HealthProbesLivenessTimeout: 60
HealthProbesReadinessTimeout: 60
HealthProbeFailureThreshold: 12
HealthPeriodScans: 10

nodeSelector: {}

tolerations: []

## Affinity and anti-affinity
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

## XL Deploy data Persistent Volume for 'XL Deploy export'
## If "Persistence.Enabled" set to "false", it will use hostPath.
Persistence:
  Enabled: true
  ## Choose storage class provided by your cloud provider, example "ssd" on GKE, AWS and OpenStack
  StorageClass: "-"
  Annotations: {}
  AccessMode: ReadWriteOnce
  XldMasterPvcSize: 10Gi
  XldWorkerPvcSize: 10Gi

# Set "satellite.Enabled" value to "true" to use the "XL Satellite" feature
satellite:
  Enabled: false

keycloak:
  install: true
  extraEnv: |
    - name: PROXY_ADDRESS_FORWARDING
      value: "true"
    - name: KEYCLOAK_USER
      value: admin
    - name: KEYCLOAK_PASSWORD
      value: admin
    - name: JGROUPS_DISCOVERY_PROTOCOL
      value: dns.DNS_PING
    - name: JGROUPS_DISCOVERY_PROPERTIES
      value: 'dns_query={{ include "keycloak.serviceDnsName" . }}'
    - name: CACHE_OWNERS_COUNT
      value: "2"
    - name: CACHE_OWNERS_AUTH_SESSIONS_COUNT
      value: "2"
    - name: JAVA_OPTS
      value: >-
        -XX:+UseContainerSupport
        -XX:MaxRAMPercentage=50.0
        -Djava.net.preferIPv4Stack=true
        -Djboss.modules.system.pkgs=$JBOSS_MODULES_SYSTEM_PKGS
        -Djava.awt.headless=true
    - name: DB_VENDOR
      value: postgres
    - name: DB_ADDR
      value: {{ .Release.Name }}-postgresql
    - name: DB_PORT
      value: "5432"
    - name: DB_DATABASE
      value: keycloak
    - name: DB_USER
      value: keycloak
    - name: DB_PASSWORD
      value: keycloak
    - name: KEYCLOAK_IMPORT
      value: /realm/digitalai-platform-realm.json
  extraVolumes: |
    - name: realm-config
      configMap:
        name: {{ .Release.Name }}-realm
  extraVolumeMounts: |
    - name: realm-config
      mountPath: "/realm/"
      readOnly: true
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: haproxy-dai-xld
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    rules:
      - host: 'keycloak.example.com'
        paths:
          - path: /
            pathType: Prefix
    tls: []
  postgresql:
    enabled: false
  service:
    type: LoadBalancer

oidc:
  enabled: true
  external: false
  clientId:
  clientSecret:
  clientAuthMethod:
  clientAuthJwt:
    enable: false
    jwsAlg:
    tokenKeyId:
    keyStore:
      enable: false
      path:
      password:
      type:
    key:
      enable: false
      alias:
      password:
  issuer:
  keyRetrievalUri:
  accessTokenUri:
  userAuthorizationUri:
  logoutUri:
  redirectUri:
  postLogoutRedirectUri:
  rolesClaimName:
  userNameClaimName:
  scopes:
  idTokenJWSAlg:
  accessToken:
    enable: false
    issuer:
    audience:
    keyRetrievalUri:
    jwsAlg:
    secretKey:
  loginMethodDescription:
  proxyHost:
  proxyPort:
centralConfiguration:
  replicas: 1
  image:
    repository: xebialabs/central-configuration
    tag: ""
  persistence:
    annotations: { }
    pvcSize: 500M
  migrateFromEmbedded: "false"
